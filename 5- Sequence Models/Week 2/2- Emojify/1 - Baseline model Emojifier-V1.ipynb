{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## imports and configuration\n",
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as pt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pt.set_printoptions(linewidth=200)\n",
    "device = pt.device(\"cuda:0\" if pt.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Baseline model: Emojifier-V1\n",
    "\n",
    "### 1.1 - Dataset EMOJISET\n",
    "\n",
    "Let's start by building a simple baseline classifier. \n",
    "\n",
    "You have a tiny dataset (X, Y) where:\n",
    "- X contains 127 sentences (strings)\n",
    "- Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "<img src=\"images/data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 1**: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
    "\n",
    "Let's load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to print sentences from X_train and corresponding labels from Y_train. Change `index` to see different examples. Because of the font the iPython notebook uses, the heart emoji may be colored black rather than red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am proud of your achievements üòÑ\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the Emojifier-V1\n",
    "\n",
    "In this part, you are going to implement a baseline model called \"Emojifier-v1\".  \n",
    "\n",
    "<center>\n",
    "<img src=\"images/image_1.png\" style=\"width:900px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Baseline model (Emojifier-V1).</center></caption>\n",
    "</center>\n",
    "\n",
    "The input of the model is a string corresponding to a sentence (e.g. \"I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape  current shape $(m, 1)$ into a \"one-hot representation\" $(m, 5)$, where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, `Y_oh` stands for \"Y-one-hot\" in the variable names `Y_oh_train` and `Y_oh_test`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is converted into one hot [1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = 50\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data is now ready to be fed into the Emojify-V2 model. Let's implement the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Implementing Emojifier-V1\n",
    "\n",
    "As shown in Figure (2), the first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. Run the following cell to load the `word_to_vec_map`, which contains all the vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've loaded:\n",
    "- `word_to_index`: dictionary mapping from words to their indices in the vocabulary (400,001 words, with the valid indices ranging from 0 to 400,000)\n",
    "- `index_to_word`: dictionary mapping from indices to their corresponding words in the vocabulary\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n",
    "\n",
    "Run the following cell to check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `sentence_to_avg()`. You will need to carry out two steps:\n",
    "1. Convert every sentence to lower-case, then split the sentence into a list of words. `X.lower()` and `X.split()` might be useful. \n",
    "2. For each word in the sentence, access its GloVe representation. Then, average all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    words = sentence.lower().split()\n",
    "    avg = pt.zeros(50, dtype=pt.float32)\n",
    "    for w in words:\n",
    "        avg += pt.tensor(word_to_vec_map[w], dtype=pt.float32)\n",
    "    avg = avg/len(words)\n",
    "\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Emo_Dataset(Dataset):\n",
    "    def __init__(self, X, Y, word_to_vec_map):\n",
    "        self.word_to_vec_map = word_to_vec_map\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        super().__init__()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = sentence_to_avg(self.X[index], word_to_vec_map)\n",
    "        y = self.Y[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = Emo_Dataset(X_train, Y_train, word_to_vec_map)\n",
    "trn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True)\n",
    "test_ds = Emo_Dataset(X_test, Y_test, word_to_vec_map)\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "You now have all the pieces to finish implementing the `model()` function.\n",
    "\n",
    "**Exercise**: Implement the `model()` function described in Figure (2). Assuming here that $Yoh$ (\"Y one hot\") is the one-hot encoding of the output labels, the equations you need to implement in the forward pass and to compute the cross-entropy cost are:\n",
    "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
    "$$ a^{(i)} = softmax(z^{(i)})$$\n",
    "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$\n",
    "\n",
    "It is possible to come up with a more efficient vectorized implementation. But since we are using a for-loop to convert the sentences one at a time into the avg^{(i)} representation anyway, let's not bother this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(in_features=50, out_features=5).to(device)\n",
    "nn.init.xavier_uniform_(model.weight)  # Initialize parameters using Xavier initialization\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dl):\n",
    "    model.eval()\n",
    "    num_examples = len(dl) #length of the data loader is equal to the number of examples since it has a batch size of 1\n",
    "    correct_preds = 0\n",
    "    with pt.no_grad():\n",
    "        for x, y in dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            z = model(x)\n",
    "            pred_cls = pt.softmax(z, dim=-1).argmax()\n",
    "            correct_preds += (y == pred_cls).item()\n",
    "    return correct_preds / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, trn_dl, num_epochs=400):\n",
    "    n_y = 5         #number of classes\n",
    "    n_h = 50        #dimensions of the GloVe vectors\n",
    "    model.train()\n",
    "    for e in range(num_epochs):\n",
    "        for x, y in trn_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            z = model(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if e % 100 == 0:\n",
    "            print(f'Epoch: {e} --- cost = {loss}')\n",
    "            model.eval()\n",
    "            accuracy = compute_accuracy(model, trn_dl)\n",
    "            print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.6938440799713135\n",
      "Accuracy: 0.3106060606060606\n",
      "Epoch: 100 --- cost = 0.20274090766906738\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 200 --- cost = 0.12577247619628906\n",
      "Accuracy: 0.9545454545454546\n",
      "Epoch: 300 --- cost = 0.025539398193359375\n",
      "Accuracy: 0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "train(model, loss_fn, optimizer, trn_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Your model has pretty high accuracy on the training set. Lets now see how it does on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9696969696969697\n",
      "Test set accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set accuracy:\", compute_accuracy(model, trn_dl))\n",
    "print(\"Test set accuracy:\", compute_accuracy(model, test_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random guessing would have had 20% accuracy given that there are 5 classes. This is pretty good performance after training on only 127 examples. \n",
    "\n",
    "In the training set, the algorithm saw the sentence \"*I love you*\" with the label ‚ù§Ô∏è. You can check however that the word \"adore\" does not appear in the training set. Nonetheless, lets see what happens if you write \"*I adore you*.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_custom_sentences(model, sentences, labels, word_to_vec_map):\n",
    "    model.eval()\n",
    "    num_examples = len(sentences)\n",
    "    correct_preds = 0\n",
    "    y = pt.tensor(labels).to(device)\n",
    "    with pt.no_grad():\n",
    "        for (sentence, label) in zip(sentences, y):\n",
    "            x = sentence_to_avg(sentence, word_to_vec_map).to(device)\n",
    "            z = model(x)\n",
    "            pred_cls = pt.softmax(z, dim=-1).argmax()\n",
    "            correct_preds += (label == pred_cls).item()\n",
    "            print(sentence, label_to_emoji(pred_cls.item()))\n",
    "    \n",
    "    \n",
    "    print('\\nAccuracy:', correct_preds/num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_my_sentences = [\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"]\n",
    "Y_my_labels = [0, 0, 2, 1, 4, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i adore you ‚ù§Ô∏è\n",
      "i love you ‚ù§Ô∏è\n",
      "funny lol üòÑ\n",
      "lets play with a ball ‚öæ\n",
      "food is ready üç¥\n",
      "not feeling happy üòû\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_custom_sentences(model, X_my_sentences, Y_my_labels, word_to_vec_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-0.4",
   "language": "python",
   "name": "pytorch-0.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
